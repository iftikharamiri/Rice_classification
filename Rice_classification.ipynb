{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5372aabc",
   "metadata": {},
   "source": [
    "# COMP5318 Assignment 1: Rice Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0a96b5",
   "metadata": {},
   "source": [
    "Student 2 = Iftikhar Amiri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d32a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1313d627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3173c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the rice dataset: rice-final2.csv\n",
    "rice_data = pd.read_csv('rice-final2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65200a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process dataset\n",
    "def load_and_preprocess_data(file_path):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Fill missing values with mean\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "    # Convert '?' to NaN for numeric columns\n",
    "    df.replace('?', np.nan, inplace=True)\n",
    "\n",
    "    # Ensure numerical columns are float\n",
    "    for col in df.columns[:-1]:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    df[df.columns[:-1]] = imputer.fit_transform(df[df.columns[:-1]])\n",
    "\n",
    "    # Normalise data using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    df[df.columns[:-1]] = scaler.fit_transform(df[df.columns[:-1]])\n",
    "\n",
    "    # Change class values ('class1' -> 0, 'class2' -> 1)\n",
    "    df[df.columns[-1]] = df[df.columns[-1]].map({'class1': 0, 'class2': 1})\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd72add5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0621,0.4999,0.5410,0.2079,0.2594,0.0613,0\n",
      "0.8073,0.7474,0.6721,0.2634,0.2038,0.0586,0\n",
      "0.3105,0.6030,0.4187,0.0000,0.0000,0.0900,0\n",
      "0.3105,0.5618,0.6148,0.3604,0.0000,0.0950,0\n",
      "0.1863,0.8144,0.6230,0.4990,0.4539,0.1597,1\n",
      "0.1863,0.6039,0.4754,0.1525,0.1000,0.0655,1\n",
      "0.6832,0.7114,0.6230,0.0000,0.0000,0.0877,1\n",
      "0.5589,0.5258,0.6230,0.5129,0.0000,0.0869,0\n",
      "0.1242,0.4639,0.5574,0.5822,0.0000,0.1009,1\n",
      "0.2484,0.5722,0.5902,0.6515,0.3835,0.0979,0\n"
     ]
    }
   ],
   "source": [
    "# Print first ten rows of pre-processed dataset to 4 decimal places as per assignment spec\n",
    "# A function is provided to assist\n",
    "\n",
    "def print_data(X, y, n_rows=10):\n",
    "    \"\"\"Takes a numpy data array and target and prints the first ten rows.\n",
    "    \n",
    "    Arguments:\n",
    "        X: numpy array of shape (n_examples, n_features)\n",
    "        y: numpy array of shape (n_examples)\n",
    "        n_rows: numpy of rows to print\n",
    "    \"\"\"\n",
    "    for example_num in range(n_rows):\n",
    "        for feature in X[example_num]:\n",
    "            print(\"{:.4f}\".format(feature), end=\",\")\n",
    "\n",
    "        if example_num == len(X)-1:\n",
    "            print(y[example_num],end=\"\")\n",
    "        else:\n",
    "            print(y[example_num])\n",
    "\n",
    "# Importing test-before dataset\n",
    "file_path = 'test-before.csv'\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "processed_df = load_and_preprocess_data(file_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = processed_df.iloc[:, :-1].values\n",
    "y = processed_df.iloc[:, -1].values\n",
    "\n",
    "# Print the first 10 rows of the preprocessed dataset\n",
    "print_data(X, y, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0559d165",
   "metadata": {},
   "source": [
    "### Part 1: Cross-validation without parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd920ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the 10 fold stratified cross-validation\n",
    "cvKFold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "# The stratified folds from cvKFold should be provided to the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f725c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "def logregClassifier(X, y):\n",
    "    # Create a logistic regression classifier\n",
    "    lrclf = LogisticRegression()\n",
    "    # Perform cross-validation using the logistic regression model\n",
    "    scores = cross_val_score(lrclf, X, y, cv=cvKFold)\n",
    "    # Return the mean accuracy score from cross-validation\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd7532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naïve Bayes\n",
    "def nbClassifier(X, y):\n",
    "    nbclf = GaussianNB()\n",
    "    scores = cross_val_score(nbclf, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357db4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "def dtClassifier(X, y):\n",
    "    dtclf = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "    scores = cross_val_score(dtclf, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d2318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembles: Bagging, Ada Boost and Gradient Boosting\n",
    "def bagDTClassifier(X, y, n_estimators, max_samples, max_depth):\n",
    "    # Create a base Decision Tree classifier with entropy as the criterion for splitting\n",
    "    base_estimator = DecisionTreeClassifier(criterion='entropy', max_depth=max_depth)\n",
    "    # Create a Bagging classifier using the specified base estimator\n",
    "    bdtclf = BaggingClassifier(estimator=base_estimator, \n",
    "                              n_estimators=n_estimators, \n",
    "                              max_samples=max_samples, \n",
    "                              random_state=0)\n",
    "    scores = cross_val_score(bdtclf, X, y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "def adaDTClassifier(X, y, n_estimators, learning_rate, max_depth):\n",
    "    base_estimator = DecisionTreeClassifier(criterion='entropy', max_depth=max_depth)\n",
    "    adtclf = AdaBoostClassifier(estimator=base_estimator, \n",
    "                               n_estimators=n_estimators, \n",
    "                               learning_rate=learning_rate, \n",
    "                               random_state=0)\n",
    "    scores = cross_val_score(adtclf, X, y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "def gbClassifier(X, y, n_estimators, learning_rate):\n",
    "    gbclf = GradientBoostingClassifier(n_estimators=n_estimators, \n",
    "                                      learning_rate=learning_rate, \n",
    "                                      random_state=0)\n",
    "    scores = cross_val_score(gbclf, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c58699",
   "metadata": {},
   "source": [
    "### Part 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7fabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogR average cross-validation accuracy:0.6700\n",
      "NB average cross-validation accuracy:0.6555\n",
      "DT average cross-validation accuracy:0.7702\n",
      "Bagging average cross-validation accuracy:0.7514\n",
      "AdaBoost average cross-validation accuracy:0.7562\n",
      "GB average cross-validation accuracy:0.7464\n"
     ]
    }
   ],
   "source": [
    "# Parameters for Part 1:\n",
    "\n",
    "#Bagging\n",
    "bag_n_estimators = 50\n",
    "bag_max_samples = 100\n",
    "bag_max_depth = 5\n",
    "\n",
    "#AdaBoost\n",
    "ada_n_estimators = 50\n",
    "ada_learning_rate = 0.5\n",
    "ada_bag_max_depth = 5\n",
    "\n",
    "#GB\n",
    "gb_n_estimators = 50\n",
    "gb_learning_rate = 0.5\n",
    "\n",
    "# Compute the accuracy for each classifier using cross-validation\n",
    "lr_accuracy = logregClassifier(X, y)\n",
    "nb_accuracy = nbClassifier(X, y)\n",
    "dt_accuracy = dtClassifier(X,y)\n",
    "bag_accuracy = bagDTClassifier(X, y, bag_n_estimators, bag_max_samples, bag_max_depth)\n",
    "ada_accuracy = adaDTClassifier(X, y, ada_n_estimators, ada_learning_rate, ada_bag_max_depth)\n",
    "gb_accuracy = gbClassifier(X, y, gb_n_estimators, gb_learning_rate)\n",
    "\n",
    "# Print results for each classifier in part 1 to 4 decimal places here:\n",
    "print(f\"LogR average cross-validation accuracy:{lr_accuracy:.4f}\")\n",
    "print(f\"NB average cross-validation accuracy:{nb_accuracy:.4f}\")\n",
    "print(f\"DT average cross-validation accuracy:{dt_accuracy:.4f}\")\n",
    "print(f\"Bagging average cross-validation accuracy:{bag_accuracy:.4f}\")\n",
    "print(f\"AdaBoost average cross-validation accuracy:{ada_accuracy:.4f}\")\n",
    "print(f\"GB average cross-validation accuracy:{gb_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1deb65",
   "metadata": {},
   "source": [
    "### Part 2: Cross-validation with parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f163070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "k = [1, 3, 5, 7]\n",
    "p = [1, 2]\n",
    "\n",
    "def bestKNNClassifier(X, y):\n",
    "    # create the training and test splits\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "    \n",
    "    # Create parameter grid\n",
    "    param_grid = {\n",
    "        'n_neighbors': k,\n",
    "        'p': p\n",
    "    }\n",
    "        \n",
    "    knclf = KNeighborsClassifier()\n",
    "    \n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(knclf, param_grid=param_grid, \n",
    "                              cv=cvKFold, scoring='accuracy')\n",
    "    \n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get best parameters and score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_cv_accuracy = grid_search.best_score_\n",
    "    \n",
    "    # Get test set accuracy\n",
    "    test_accuracy = accuracy_score(y_test, grid_search.best_estimator_.predict(X_test))\n",
    "    \n",
    "    return best_params, best_cv_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1ab019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "# You should use SVC from sklearn.svm with kernel set to 'rbf'\n",
    "C = [0.01, 0.1, 1, 5] \n",
    "gamma = [0.01, 0.1, 1, 10]\n",
    "\n",
    "def bestSVMClassifier(X, y):\n",
    "    # create the training and test splits\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "    \n",
    "    # Create parameter grid with updated ranges\n",
    "    param_grid = {\n",
    "        'C': C,\n",
    "        'gamma': gamma,\n",
    "        'kernel': ['rbf']\n",
    "    }\n",
    "    \n",
    "    # Create SVM classifier\n",
    "    model = SVC()\n",
    "    \n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                              cv=cvKFold, scoring='accuracy')\n",
    "    \n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get best parameters and score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_cv_accuracy = grid_search.best_score_\n",
    "    \n",
    "    # Get test set accuracy\n",
    "    test_accuracy = accuracy_score(y_test, grid_search.best_estimator_.predict(X_test))\n",
    "    \n",
    "    return best_params, best_cv_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e47e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# You should use RandomForestClassifier from sklearn.ensemble with information gain and max_features set to ‘sqrt’.\n",
    "n_estimators = [10, 30, 60, 100]\n",
    "max_leaf_nodes = [6, 12]\n",
    "\n",
    "def bestRFClassifier(X, y):\n",
    "    # create the training and test splits\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "    \n",
    "    # Create parameter grid with updated ranges\n",
    "    param_grid = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_leaf_nodes': max_leaf_nodes\n",
    "    }\n",
    "    \n",
    "    # Create Random Forest classifier with information gain (entropy) and max_features='sqrt'\n",
    "    model = RandomForestClassifier(criterion='entropy', max_features='sqrt', random_state=0)\n",
    "    \n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                              cv=cvKFold, scoring='accuracy')\n",
    "    \n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get best parameters and score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_cv_accuracy = grid_search.best_score_\n",
    "    \n",
    "    # Get test set accuracy and F1 scores\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    return best_params, best_cv_accuracy, test_accuracy, macro_f1, weighted_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21f03b",
   "metadata": {},
   "source": [
    "### Part 2: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81235b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN best k: 1\n",
      "KNN best p: 1\n",
      "KNN cross-validation accuracy: 0.7329\n",
      "KNN test set accuracy: 0.6415\n",
      "\n",
      "SVM best C: 5.0000\n",
      "SVM best gamma: 10.0000\n",
      "SVM cross-validation accuracy: 0.6858\n",
      "SVM test set accuracy: 0.5849\n",
      "\n",
      "RF best n_estimators: 60\n",
      "RF best max_leaf_nodes: 12\n",
      "RF cross-validation accuracy: 0.7883\n",
      "RF test set accuracy: 0.6981\n",
      "RF test set macro average F1: 0.6845\n",
      "RF test set weighted average F1: 0.6956\n"
     ]
    }
   ],
   "source": [
    "# Perform Grid Search with 10-fold stratified cross-validation (GridSearchCV in sklearn). \n",
    "# The stratified folds from cvKFold should be provided to GridSearchV\n",
    "\n",
    "# This should include using train_test_split from sklearn.model_selection with stratification and random_state=0\n",
    "# Print results for each classifier here. All results should be printed to 4 decimal places except for\n",
    "# \"k\", \"p\", n_estimators\" and \"max_leaf_nodes\" which should be printed as integers.\n",
    "best_params_knn, knn_cv_acc, knn_test_acc = bestKNNClassifier(X, y)\n",
    "print(f\"KNN best k: {best_params_knn['n_neighbors']}\")\n",
    "print(f\"KNN best p: {best_params_knn['p']}\")\n",
    "print(f\"KNN cross-validation accuracy: {knn_cv_acc:.4f}\")\n",
    "print(f\"KNN test set accuracy: {knn_test_acc:.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "best_params_svm, svm_cv_acc, svm_test_acc = bestSVMClassifier(X, y)\n",
    "print(f\"SVM best C: {best_params_svm['C']:.4f}\")\n",
    "print(f\"SVM best gamma: {best_params_svm['gamma']:.4f}\")\n",
    "print(f\"SVM cross-validation accuracy: {svm_cv_acc:.4f}\")\n",
    "print(f\"SVM test set accuracy: {svm_test_acc:.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "best_params_rf, rf_cv_acc, rf_test_acc, rf_macro_f1, rf_weighted_f1 = bestRFClassifier(X, y)\n",
    "print(f\"RF best n_estimators: {best_params_rf['n_estimators']}\")\n",
    "print(f\"RF best max_leaf_nodes: {best_params_rf['max_leaf_nodes']}\")\n",
    "print(f\"RF cross-validation accuracy: {rf_cv_acc:.4f}\")\n",
    "print(f\"RF test set accuracy: {rf_test_acc:.4f}\")\n",
    "print(f\"RF test set macro average F1: {rf_macro_f1:.4f}\")\n",
    "print(f\"RF test set weighted average F1: {rf_weighted_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c701d4e",
   "metadata": {},
   "source": [
    "### Part 3: Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bead2685",
   "metadata": {},
   "source": [
    "##### Write one paragraph describing the most important thing that you have learned throughout this assignment.\n",
    "##### **Student 1:** The key takeaway from this assignment is the importance of systematic data preparation and model selection in improving machine learning performance. The most important thing I have learned that cleaning the data by imputing missing values with the mean helps maintain data integrity. Hence, normalizing the data using MinMaxScaler from the sklearn library ensures consistency across different feature scales before testing. I also discovered that stratified splitting in Part 1 is crucial for preserving class distributions and reducing bias in the training and testing sets. In Part 2, I explored cross-validation with parameter tuning such as adjusting C and gamma for SVM or n_estimators for ensemble methods by using tools like GridSearchCV. This approach significantly improves model performance compared to using the default settings. Beyond accuracy, metrics like macro-F1 and weighted-F1 scores from the Random Forest provide a better evaluation of class-aware performance, especially for imbalanced or multi-class data. In summary, stratified cross-validation and hyperparameter tuning are just as important as model selection in achieving accurate rice classification that can be employed in the real-world application.\n",
    "##### **Student 2:** Throughout this assignment, one of the most valuable lessons I've learned is the importance of carefully preparing data before applying machine learning algorithms. I've gained practical skills in essential steps such as dividing data into training and test sets and adjusting hyperparameters to boost model accuracy. Additionally, working on this assignment greatly improved my familiarity with powerful tools in scikit-learn, especially GridSearchCV. I found the concept behind AdaBoost particularly fascinating; its approach of sequentially learning by focusing on mistakes from previous iterations was insightful and impressive. Overall, this experience clearly demonstrated how machine learning techniques can effectively address real-world problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d64622d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
